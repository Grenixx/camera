<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Hand Gesture Detection</title>
  <style>
    body {
      font-family: Roboto, sans-serif;
      margin: 0;
      padding: 1em;
      background-color: #f4f4f4;
      color: #333;
    }
    h1 {
      color: #007f8b;
      text-align: center;
    }
    #videoContainer {
      position: relative;
      width: 100%;
      max-width: 640px;
      margin: auto;
    }
    video, canvas {
      width: 100%;
      transform: scaleX(-1);
    }
    #gesture_output {
      margin-top: 1em;
      padding: 1em;
      background-color: #007f8b;
      color: white;
      font-size: 1.1em;
      white-space: pre-line;
    }
  </style>
</head>
<body>
  <h1>Hand Gesture Recognition</h1>
  <div id="videoContainer">
    <video id="webcam" autoplay playsinline></video>
    <canvas id="output_canvas"></canvas>
  </div>
  <div id="gesture_output"></div>

  <script type="module">
    import {
      GestureRecognizer,
      FilesetResolver,
      DrawingUtils
    } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";

    const video = document.getElementById("webcam");
    const canvas = document.getElementById("output_canvas");
    const canvasCtx = canvas.getContext("2d");
    const gestureOutput = document.getElementById("gesture_output");
    let gestureRecognizer;
    let lastVideoTime = -1;
    let runningMode = "VIDEO";

    const setupGestureRecognizer = async () => {
      const vision = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
      );
      gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
        baseOptions: {
          modelAssetPath:
            "https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task",
          delegate: "GPU"
        },
        runningMode: runningMode
      });
      startWebcam();
    };

    const startWebcam = () => {
      navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {
        video.srcObject = stream;
        video.addEventListener("loadeddata", predictWebcam);
      });
    };

    const predictWebcam = async () => {
      if (!gestureRecognizer) return;

      let nowInMs = Date.now();
      if (video.currentTime !== lastVideoTime) {
        lastVideoTime = video.currentTime;
        const results = gestureRecognizer.recognizeForVideo(video, nowInMs);

        canvasCtx.save();
        canvasCtx.clearRect(0, 0, canvas.width, canvas.height);
        const drawingUtils = new DrawingUtils(canvasCtx);
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;

        if (results.landmarks) {
          for (const landmarks of results.landmarks) {
            drawingUtils.drawConnectors(landmarks, GestureRecognizer.HAND_CONNECTIONS, {
              color: "#00FF00", lineWidth: 5
            });
            drawingUtils.drawLandmarks(landmarks, {
              color: "#FF0000", lineWidth: 2
            });
          }
        }

        if (results.gestures.length > 0) {
          const categoryName = results.gestures[0][0].categoryName;
          const score = (results.gestures[0][0].score * 100).toFixed(2);
          const hand = results.handednesses[0][0].displayName;
          gestureOutput.innerText = `Gesture: ${categoryName}\nConfidence: ${score}%\nHand: ${hand}`;
        } else {
          gestureOutput.innerText = "";
        }

        canvasCtx.restore();
      }
      window.requestAnimationFrame(predictWebcam);
    };

    setupGestureRecognizer();
  </script>
</body>
</html>
